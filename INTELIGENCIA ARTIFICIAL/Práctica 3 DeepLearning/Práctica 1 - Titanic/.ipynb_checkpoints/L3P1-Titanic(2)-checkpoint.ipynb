{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
     ]
    }
   ],
   "source": [
    "# Aquí se importan las librerias que se van a utiliar\n",
    "\n",
    "%reset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import plot_model, np_utils\n",
    "import keras.optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para imprimir la supervivencia de los personajes\n",
    "def Imprimir_Prediccion_Vida(personajes, genero, edad, precio, probabilidad):\n",
    "    num_total = 0\n",
    "    num_mujeres = 0\n",
    "    num_niños = 0\n",
    "    num_primera = 0\n",
    "    num_hombres = 0\n",
    "    for i in range(0, personajes.shape[0]):\n",
    "        num_total = num_total+1\n",
    "        if (precio[i] >= 500):\n",
    "            num_primera = num_primera+1\n",
    "        if (edad[i] <= 15):\n",
    "            num_niños = num_niños+1\n",
    "        if (genero[i] == \"female\"):\n",
    "            num_mujeres = num_mujeres+1\n",
    "        if (genero[i] == \"male\"):\n",
    "            num_hombres = num_hombres+1\n",
    "        print(\"- \" + str(personajes[i]) + \": \" + str(round(probabilidad[i][0],2)) + \"%\")\n",
    "    print(\"\\nNúmero total de datos: \", num_total)\n",
    "    print(\"Número de mujeres: \", num_mujeres)\n",
    "    print(\"Número de hombres: \", num_hombres)\n",
    "    print(\"Número de niños: \", num_niños)\n",
    "    print(\"Número pasajeros primera clase: \", num_primera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función imprimir datos antes de la predicción\n",
    "def Imprimir_Datos (pasajeros, genero, edad, precio):\n",
    "    num_total = 0\n",
    "    num_mujeres = 0\n",
    "    num_hombres = 0\n",
    "    num_niños = 0\n",
    "    num_primera = 0\n",
    "    for i in range(0, pasajeros.shape[0]):\n",
    "        num_total = num_total+1\n",
    "        if (precio[i] >= 500):\n",
    "            num_primera = num_primera+1\n",
    "        if (edad[i] <= 15):\n",
    "            num_niños = num_niños+1\n",
    "        if (genero[i] == \"female\"):\n",
    "            num_mujeres = num_mujeres+1\n",
    "        if (genero[i] == \"male\"):\n",
    "            num_hombres = num_hombres+1\n",
    "    print(\"Número total de datos: \", num_total)\n",
    "    print(\"Número de mujeres: \", num_mujeres)\n",
    "    print(\"Número de hombres: \", num_hombres)\n",
    "    print(\"Número de niños: \", num_niños)\n",
    "    print(\"Número pasajeros primera clase: \", num_primera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función imprimir la media de los pesos\n",
    "def Imprimir_Medias_Pesos(nombres, pesos):\n",
    "    for i in range(0, nombres.shape[0]):\n",
    "        print(\"- \" + str(nombres[i]) + \": \" + str(pesos[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recolección datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los datos en el dataset: \n",
      "Número total de datos:  891\n",
      "Número de mujeres:  314\n",
      "Número de hombres:  577\n",
      "Número de niños:  83\n",
      "Número pasajeros primera clase:  3\n"
     ]
    }
   ],
   "source": [
    "# Obtención del fichero de datos de entrenamiento y validación\n",
    "datos = pd.read_csv('../Datos/titanic.csv')\n",
    "\n",
    "# Recolección de información de entrada para la red neuronal (especificada en el enunciado)\n",
    "clase = datos.loc[:,'Pclass']\n",
    "genero = datos.loc[:,'Sex']\n",
    "edad = datos.loc[:,'Age']\n",
    "tarifa = datos.loc[:,'Fare']\n",
    "puerto = datos.loc[:,'Embarked']\n",
    "\n",
    "# Eliminamos las casillas donde no existe el dato de la edad\n",
    "# Hacemos el .roud() debido a que la media de la edad sale con muchos decimales\n",
    "\n",
    "edad = datos['Age'].fillna(datos['Age'].mean()).round()\n",
    "\n",
    "# Convertimos \"male\" en 0 y \"female\" en 1 para poder hacer el entrenamiento\n",
    "genero = genero.replace('male', 0)\n",
    "genero = genero.replace('female', 1)\n",
    "\n",
    "# Convertiomos los datos de los puertos según estos datos:\n",
    "# Q ==> 0\n",
    "# S ==> 1\n",
    "# C ==> 2\n",
    "puerto = puerto.replace('Q', 0)\n",
    "puerto = puerto.replace('S', 1)\n",
    "puerto = puerto.replace('C', 2)\n",
    "\n",
    "# Unión de la información de entrada\n",
    "entrada = np.array([[clase],[genero],[edad],[tarifa],[puerto]])\n",
    "\n",
    "# Cambiamos el tamaño de la matriz\n",
    "entrada = entrada.transpose().reshape(891, 5)\n",
    "\n",
    "# Recolección de la salida esperada de la red neuronal\n",
    "salida_esperada = np.array(datos.loc[:,'Survived'])\n",
    "\n",
    "# Conjunto entrenamiento\n",
    "entrada_entrenamiento = entrada[:802,:]\n",
    "salida_entrenamiento = salida_esperada[:802]\n",
    "\n",
    "# Conjunto validación\n",
    "entrada_validacion = entrada[802:,:]\n",
    "salida_validacion = salida_esperada[802:]\n",
    "\n",
    "print(\"Los datos en el dataset: \")\n",
    "Imprimir_Datos(datos['Name'], datos['Sex'], datos['Age'], datos['Fare'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recolección datos de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtención del fichero de datos de entrenamiento\n",
    "datos_pre = pd.read_csv('../Datos/predict_titanic.csv')\n",
    "\n",
    "# Recolección de información de entrada para la red neuronal\n",
    "clase = datos_pre.loc[:,'Pclass']\n",
    "genero = datos_pre.loc[:,'Sex']\n",
    "edad = datos_pre.loc[:,'Age']\n",
    "tarifa = datos_pre.loc[:,'Fare']\n",
    "puerto = datos_pre.loc[:,'Embarked']\n",
    "    \n",
    "# Eliminamos las casillas donde no existe el dato de la edad\n",
    "edad = datos_pre['Age'].fillna(datos_pre['Age'].mean()).round()\n",
    "\n",
    "# Hacemos lo mismo con la tarifa\n",
    "tarifa = datos_pre['Fare'].fillna(datos_pre['Fare'].mean())\n",
    "\n",
    "# Convertimos \"male\" en 0 y \"female\" en 1 para poder hacer el entrenamiento\n",
    "genero = genero.replace('male', 0)\n",
    "genero = genero.replace('female', 1)\n",
    "\n",
    "# Convertiomos los datos de los puertos según estos datos:\n",
    "# Q ==> 0\n",
    "# S ==> 1\n",
    "# C ==> 2\n",
    "puerto = puerto.replace('Q', 0)\n",
    "puerto = puerto.replace('S', 1)\n",
    "puerto = puerto.replace('C', 2)\n",
    "\n",
    "# Unión de la información de entrada\n",
    "entrada_pre = np.array([[clase],[genero],[edad],[tarifa],[puerto]])\n",
    "\n",
    "# Cambiamos el tamaño\n",
    "entrada_pre = entrada_pre.transpose().reshape(418,5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación 1. Entrenamiento Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 802 samples, validate on 89 samples\n",
      "Epoch 1/250\n",
      "802/802 [==============================] - 0s 464us/step - loss: 0.2539 - acc: 0.6397 - val_loss: 0.2117 - val_acc: 0.7416\n",
      "Epoch 2/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.2489 - acc: 0.6883 - val_loss: 0.2016 - val_acc: 0.7528\n",
      "Epoch 3/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.2121 - acc: 0.6883 - val_loss: 0.1850 - val_acc: 0.6854\n",
      "Epoch 4/250\n",
      "802/802 [==============================] - 0s 49us/step - loss: 0.2026 - acc: 0.7020 - val_loss: 0.1601 - val_acc: 0.7640\n",
      "Epoch 5/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1855 - acc: 0.7207 - val_loss: 0.1935 - val_acc: 0.6966\n",
      "Epoch 6/250\n",
      "802/802 [==============================] - 0s 49us/step - loss: 0.2213 - acc: 0.6995 - val_loss: 0.1608 - val_acc: 0.7528\n",
      "Epoch 7/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1864 - acc: 0.7095 - val_loss: 0.1708 - val_acc: 0.7753\n",
      "Epoch 8/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1698 - acc: 0.7581 - val_loss: 0.1475 - val_acc: 0.7865\n",
      "Epoch 9/250\n",
      "802/802 [==============================] - 0s 51us/step - loss: 0.1606 - acc: 0.7706 - val_loss: 0.1685 - val_acc: 0.7640\n",
      "Epoch 10/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1734 - acc: 0.7494 - val_loss: 0.1866 - val_acc: 0.6966\n",
      "Epoch 11/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1608 - acc: 0.7706 - val_loss: 0.1487 - val_acc: 0.7753\n",
      "Epoch 12/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1550 - acc: 0.7830 - val_loss: 0.1680 - val_acc: 0.7640\n",
      "Epoch 13/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1680 - acc: 0.7594 - val_loss: 0.1581 - val_acc: 0.7978\n",
      "Epoch 14/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1534 - acc: 0.7805 - val_loss: 0.1473 - val_acc: 0.7640\n",
      "Epoch 15/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1650 - acc: 0.7781 - val_loss: 0.1433 - val_acc: 0.7978\n",
      "Epoch 16/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1567 - acc: 0.7731 - val_loss: 0.1410 - val_acc: 0.7753\n",
      "Epoch 17/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1539 - acc: 0.7893 - val_loss: 0.1409 - val_acc: 0.7865\n",
      "Epoch 18/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1499 - acc: 0.7918 - val_loss: 0.1395 - val_acc: 0.7865\n",
      "Epoch 19/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1517 - acc: 0.7943 - val_loss: 0.1426 - val_acc: 0.7865\n",
      "Epoch 20/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1606 - acc: 0.7681 - val_loss: 0.1419 - val_acc: 0.7865\n",
      "Epoch 21/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1468 - acc: 0.7893 - val_loss: 0.1399 - val_acc: 0.7753\n",
      "Epoch 22/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1477 - acc: 0.7880 - val_loss: 0.1569 - val_acc: 0.7865\n",
      "Epoch 23/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1591 - acc: 0.7768 - val_loss: 0.1415 - val_acc: 0.7865\n",
      "Epoch 24/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1480 - acc: 0.7905 - val_loss: 0.1380 - val_acc: 0.7865\n",
      "Epoch 25/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1475 - acc: 0.7855 - val_loss: 0.1411 - val_acc: 0.7865\n",
      "Epoch 26/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1563 - acc: 0.7793 - val_loss: 0.1389 - val_acc: 0.7865\n",
      "Epoch 27/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1561 - acc: 0.7756 - val_loss: 0.1411 - val_acc: 0.7978\n",
      "Epoch 28/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1510 - acc: 0.7880 - val_loss: 0.1408 - val_acc: 0.8090\n",
      "Epoch 29/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1476 - acc: 0.7893 - val_loss: 0.1361 - val_acc: 0.7865\n",
      "Epoch 30/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1481 - acc: 0.7843 - val_loss: 0.1375 - val_acc: 0.7865\n",
      "Epoch 31/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1714 - acc: 0.7631 - val_loss: 0.1381 - val_acc: 0.7865\n",
      "Epoch 32/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1454 - acc: 0.7855 - val_loss: 0.1362 - val_acc: 0.7865\n",
      "Epoch 33/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1485 - acc: 0.7905 - val_loss: 0.1621 - val_acc: 0.7640\n",
      "Epoch 34/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1543 - acc: 0.7756 - val_loss: 0.1405 - val_acc: 0.7865\n",
      "Epoch 35/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1742 - acc: 0.7594 - val_loss: 0.1495 - val_acc: 0.7978\n",
      "Epoch 36/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1591 - acc: 0.7793 - val_loss: 0.1374 - val_acc: 0.7978\n",
      "Epoch 37/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1427 - acc: 0.7893 - val_loss: 0.1628 - val_acc: 0.7303\n",
      "Epoch 38/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1790 - acc: 0.7519 - val_loss: 0.1447 - val_acc: 0.8090\n",
      "Epoch 39/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1484 - acc: 0.7843 - val_loss: 0.1396 - val_acc: 0.8090\n",
      "Epoch 40/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1476 - acc: 0.7893 - val_loss: 0.1364 - val_acc: 0.7865\n",
      "Epoch 41/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1470 - acc: 0.7905 - val_loss: 0.1504 - val_acc: 0.7865\n",
      "Epoch 42/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1554 - acc: 0.7930 - val_loss: 0.1361 - val_acc: 0.7978\n",
      "Epoch 43/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1451 - acc: 0.7905 - val_loss: 0.1413 - val_acc: 0.7978\n",
      "Epoch 44/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1487 - acc: 0.7781 - val_loss: 0.1327 - val_acc: 0.8090\n",
      "Epoch 45/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1523 - acc: 0.7731 - val_loss: 0.1445 - val_acc: 0.7753\n",
      "Epoch 46/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1556 - acc: 0.7781 - val_loss: 0.1319 - val_acc: 0.7865\n",
      "Epoch 47/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1436 - acc: 0.7855 - val_loss: 0.1314 - val_acc: 0.7978\n",
      "Epoch 48/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1416 - acc: 0.8055 - val_loss: 0.1329 - val_acc: 0.8090\n",
      "Epoch 49/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1482 - acc: 0.7955 - val_loss: 0.1304 - val_acc: 0.8202\n",
      "Epoch 50/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1476 - acc: 0.7868 - val_loss: 0.1312 - val_acc: 0.8315\n",
      "Epoch 51/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1426 - acc: 0.8017 - val_loss: 0.1319 - val_acc: 0.8090\n",
      "Epoch 52/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1512 - acc: 0.7781 - val_loss: 0.1362 - val_acc: 0.7978\n",
      "Epoch 53/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1418 - acc: 0.7980 - val_loss: 0.1306 - val_acc: 0.7978\n",
      "Epoch 54/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1428 - acc: 0.7955 - val_loss: 0.1297 - val_acc: 0.8202\n",
      "Epoch 55/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1533 - acc: 0.7855 - val_loss: 0.1334 - val_acc: 0.7978\n",
      "Epoch 56/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1453 - acc: 0.8005 - val_loss: 0.1286 - val_acc: 0.8427\n",
      "Epoch 57/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1390 - acc: 0.8105 - val_loss: 0.1482 - val_acc: 0.8202\n",
      "Epoch 58/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.2462 - acc: 0.6945 - val_loss: 0.1725 - val_acc: 0.7528\n",
      "Epoch 59/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1761 - acc: 0.7569 - val_loss: 0.1391 - val_acc: 0.7753\n",
      "Epoch 60/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1511 - acc: 0.7731 - val_loss: 0.1365 - val_acc: 0.8315\n",
      "Epoch 61/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1444 - acc: 0.7943 - val_loss: 0.1427 - val_acc: 0.8090\n",
      "Epoch 62/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1490 - acc: 0.7955 - val_loss: 0.1336 - val_acc: 0.8090\n",
      "Epoch 63/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1543 - acc: 0.7943 - val_loss: 0.1329 - val_acc: 0.8202\n",
      "Epoch 64/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1405 - acc: 0.7880 - val_loss: 0.1327 - val_acc: 0.8090\n",
      "Epoch 65/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1475 - acc: 0.7818 - val_loss: 0.1287 - val_acc: 0.8090\n",
      "Epoch 66/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1423 - acc: 0.8030 - val_loss: 0.1289 - val_acc: 0.8202\n",
      "Epoch 67/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1452 - acc: 0.8155 - val_loss: 0.1379 - val_acc: 0.8090\n",
      "Epoch 68/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1520 - acc: 0.7868 - val_loss: 0.1265 - val_acc: 0.8202\n",
      "Epoch 69/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1472 - acc: 0.8030 - val_loss: 0.1486 - val_acc: 0.7978\n",
      "Epoch 70/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1461 - acc: 0.7930 - val_loss: 0.1353 - val_acc: 0.8090\n",
      "Epoch 71/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1620 - acc: 0.7793 - val_loss: 0.1254 - val_acc: 0.8202\n",
      "Epoch 72/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1406 - acc: 0.7930 - val_loss: 0.1249 - val_acc: 0.8202\n",
      "Epoch 73/250\n",
      "802/802 [==============================] - 0s 43us/step - loss: 0.1424 - acc: 0.7893 - val_loss: 0.1274 - val_acc: 0.8090\n",
      "Epoch 74/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1402 - acc: 0.8017 - val_loss: 0.1544 - val_acc: 0.7865\n",
      "Epoch 75/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.2245 - acc: 0.7132 - val_loss: 0.1437 - val_acc: 0.7978\n",
      "Epoch 76/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1931 - acc: 0.7307 - val_loss: 0.1352 - val_acc: 0.8090\n",
      "Epoch 77/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1584 - acc: 0.7693 - val_loss: 0.1326 - val_acc: 0.7865\n",
      "Epoch 78/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1635 - acc: 0.7668 - val_loss: 0.1441 - val_acc: 0.8202\n",
      "Epoch 79/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1617 - acc: 0.7930 - val_loss: 0.1312 - val_acc: 0.7978\n",
      "Epoch 80/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1452 - acc: 0.7955 - val_loss: 0.1292 - val_acc: 0.8090\n",
      "Epoch 81/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1394 - acc: 0.8067 - val_loss: 0.1552 - val_acc: 0.7865\n",
      "Epoch 82/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1591 - acc: 0.7843 - val_loss: 0.1386 - val_acc: 0.7978\n",
      "Epoch 83/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1405 - acc: 0.8092 - val_loss: 0.1281 - val_acc: 0.8090\n",
      "Epoch 84/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1393 - acc: 0.8067 - val_loss: 0.1267 - val_acc: 0.8315\n",
      "Epoch 85/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1627 - acc: 0.7868 - val_loss: 0.1328 - val_acc: 0.8090\n",
      "Epoch 86/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1539 - acc: 0.8017 - val_loss: 0.1325 - val_acc: 0.8090\n",
      "Epoch 87/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1515 - acc: 0.7943 - val_loss: 0.1258 - val_acc: 0.8090\n",
      "Epoch 88/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1387 - acc: 0.8180 - val_loss: 0.1257 - val_acc: 0.8202\n",
      "Epoch 89/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1371 - acc: 0.8155 - val_loss: 0.1241 - val_acc: 0.8202\n",
      "Epoch 90/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1388 - acc: 0.8130 - val_loss: 0.1254 - val_acc: 0.8202\n",
      "Epoch 91/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1392 - acc: 0.8042 - val_loss: 0.1252 - val_acc: 0.8315\n",
      "Epoch 92/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1416 - acc: 0.8030 - val_loss: 0.1261 - val_acc: 0.8090\n",
      "Epoch 93/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1388 - acc: 0.8092 - val_loss: 0.1272 - val_acc: 0.8202\n",
      "Epoch 94/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1485 - acc: 0.7930 - val_loss: 0.1262 - val_acc: 0.8202\n",
      "Epoch 95/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1518 - acc: 0.8055 - val_loss: 0.1258 - val_acc: 0.8090\n",
      "Epoch 96/250\n",
      "802/802 [==============================] - 0s 43us/step - loss: 0.1384 - acc: 0.8105 - val_loss: 0.1255 - val_acc: 0.7978\n",
      "Epoch 97/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1379 - acc: 0.8167 - val_loss: 0.1261 - val_acc: 0.8202\n",
      "Epoch 98/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1365 - acc: 0.8155 - val_loss: 0.1219 - val_acc: 0.8090\n",
      "Epoch 99/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1382 - acc: 0.8167 - val_loss: 0.1330 - val_acc: 0.8202\n",
      "Epoch 100/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1430 - acc: 0.7980 - val_loss: 0.1224 - val_acc: 0.8202\n",
      "Epoch 101/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1449 - acc: 0.8042 - val_loss: 0.1220 - val_acc: 0.8090\n",
      "Epoch 102/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1383 - acc: 0.8180 - val_loss: 0.1212 - val_acc: 0.8315\n",
      "Epoch 103/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1361 - acc: 0.8155 - val_loss: 0.1239 - val_acc: 0.8090\n",
      "Epoch 104/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1413 - acc: 0.8105 - val_loss: 0.1230 - val_acc: 0.8202\n",
      "Epoch 105/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1394 - acc: 0.8180 - val_loss: 0.1302 - val_acc: 0.8202\n",
      "Epoch 106/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1454 - acc: 0.8042 - val_loss: 0.1228 - val_acc: 0.8202\n",
      "Epoch 107/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1376 - acc: 0.8142 - val_loss: 0.1232 - val_acc: 0.8202\n",
      "Epoch 108/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1393 - acc: 0.8017 - val_loss: 0.1211 - val_acc: 0.8202\n",
      "Epoch 109/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1403 - acc: 0.8192 - val_loss: 0.1251 - val_acc: 0.8090\n",
      "Epoch 110/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1467 - acc: 0.7980 - val_loss: 0.1223 - val_acc: 0.8202\n",
      "Epoch 111/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1383 - acc: 0.8092 - val_loss: 0.1259 - val_acc: 0.8202\n",
      "Epoch 112/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1369 - acc: 0.8130 - val_loss: 0.1197 - val_acc: 0.8315\n",
      "Epoch 113/250\n",
      "802/802 [==============================] - 0s 49us/step - loss: 0.1360 - acc: 0.8192 - val_loss: 0.1229 - val_acc: 0.8090\n",
      "Epoch 114/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1334 - acc: 0.8229 - val_loss: 0.1226 - val_acc: 0.8090\n",
      "Epoch 115/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1375 - acc: 0.8142 - val_loss: 0.1219 - val_acc: 0.8202\n",
      "Epoch 116/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1340 - acc: 0.8217 - val_loss: 0.1223 - val_acc: 0.8202\n",
      "Epoch 117/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1331 - acc: 0.8167 - val_loss: 0.1208 - val_acc: 0.8202\n",
      "Epoch 118/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1381 - acc: 0.8092 - val_loss: 0.1362 - val_acc: 0.8090\n",
      "Epoch 119/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1427 - acc: 0.8030 - val_loss: 0.1225 - val_acc: 0.8202\n",
      "Epoch 120/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1409 - acc: 0.8055 - val_loss: 0.1185 - val_acc: 0.8315\n",
      "Epoch 121/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802/802 [==============================] - 0s 41us/step - loss: 0.1408 - acc: 0.8105 - val_loss: 0.1184 - val_acc: 0.8202\n",
      "Epoch 122/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1393 - acc: 0.8005 - val_loss: 0.1212 - val_acc: 0.8315\n",
      "Epoch 123/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1375 - acc: 0.8142 - val_loss: 0.1250 - val_acc: 0.8202\n",
      "Epoch 124/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.2083 - acc: 0.7394 - val_loss: 0.1682 - val_acc: 0.7978\n",
      "Epoch 125/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1877 - acc: 0.7494 - val_loss: 0.1499 - val_acc: 0.7978\n",
      "Epoch 126/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1567 - acc: 0.7918 - val_loss: 0.1282 - val_acc: 0.8090\n",
      "Epoch 127/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1476 - acc: 0.8080 - val_loss: 0.1210 - val_acc: 0.8090\n",
      "Epoch 128/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1436 - acc: 0.8092 - val_loss: 0.1182 - val_acc: 0.8202\n",
      "Epoch 129/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1420 - acc: 0.8105 - val_loss: 0.1183 - val_acc: 0.8315\n",
      "Epoch 130/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1367 - acc: 0.8142 - val_loss: 0.1204 - val_acc: 0.8090\n",
      "Epoch 131/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1347 - acc: 0.8192 - val_loss: 0.1175 - val_acc: 0.8202\n",
      "Epoch 132/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1348 - acc: 0.8117 - val_loss: 0.1171 - val_acc: 0.8427\n",
      "Epoch 133/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1436 - acc: 0.8030 - val_loss: 0.1306 - val_acc: 0.8202\n",
      "Epoch 134/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1357 - acc: 0.8192 - val_loss: 0.1218 - val_acc: 0.8315\n",
      "Epoch 135/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1329 - acc: 0.8204 - val_loss: 0.1189 - val_acc: 0.8202\n",
      "Epoch 136/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1343 - acc: 0.8242 - val_loss: 0.1163 - val_acc: 0.8315\n",
      "Epoch 137/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1426 - acc: 0.7993 - val_loss: 0.1202 - val_acc: 0.8315\n",
      "Epoch 138/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1407 - acc: 0.8055 - val_loss: 0.1197 - val_acc: 0.8202\n",
      "Epoch 139/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1320 - acc: 0.8204 - val_loss: 0.1231 - val_acc: 0.8315\n",
      "Epoch 140/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1401 - acc: 0.8080 - val_loss: 0.1306 - val_acc: 0.8427\n",
      "Epoch 141/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1511 - acc: 0.7955 - val_loss: 0.1198 - val_acc: 0.8202\n",
      "Epoch 142/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1385 - acc: 0.8130 - val_loss: 0.1225 - val_acc: 0.8427\n",
      "Epoch 143/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1458 - acc: 0.7980 - val_loss: 0.1196 - val_acc: 0.8652\n",
      "Epoch 144/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1325 - acc: 0.8242 - val_loss: 0.1183 - val_acc: 0.8427\n",
      "Epoch 145/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1319 - acc: 0.8192 - val_loss: 0.1168 - val_acc: 0.8315\n",
      "Epoch 146/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1330 - acc: 0.8217 - val_loss: 0.1173 - val_acc: 0.8315\n",
      "Epoch 147/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1338 - acc: 0.8105 - val_loss: 0.1227 - val_acc: 0.8202\n",
      "Epoch 148/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1322 - acc: 0.8217 - val_loss: 0.1160 - val_acc: 0.8427\n",
      "Epoch 149/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1321 - acc: 0.8192 - val_loss: 0.1197 - val_acc: 0.8315\n",
      "Epoch 150/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1343 - acc: 0.8180 - val_loss: 0.1227 - val_acc: 0.8427\n",
      "Epoch 151/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1410 - acc: 0.8030 - val_loss: 0.1206 - val_acc: 0.8315\n",
      "Epoch 152/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1365 - acc: 0.8192 - val_loss: 0.1165 - val_acc: 0.8427\n",
      "Epoch 153/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1393 - acc: 0.8042 - val_loss: 0.1290 - val_acc: 0.8202\n",
      "Epoch 154/250\n",
      "802/802 [==============================] - ETA: 0s - loss: 0.1600 - acc: 0.750 - 0s 41us/step - loss: 0.1448 - acc: 0.8130 - val_loss: 0.1245 - val_acc: 0.8202\n",
      "Epoch 155/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1454 - acc: 0.7968 - val_loss: 0.1256 - val_acc: 0.8315\n",
      "Epoch 156/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1307 - acc: 0.8192 - val_loss: 0.1230 - val_acc: 0.8427\n",
      "Epoch 157/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1308 - acc: 0.8292 - val_loss: 0.1182 - val_acc: 0.8427\n",
      "Epoch 158/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1491 - acc: 0.7943 - val_loss: 0.1182 - val_acc: 0.8427\n",
      "Epoch 159/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1334 - acc: 0.8180 - val_loss: 0.1172 - val_acc: 0.8315\n",
      "Epoch 160/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1319 - acc: 0.8167 - val_loss: 0.1153 - val_acc: 0.8427\n",
      "Epoch 161/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1328 - acc: 0.8192 - val_loss: 0.1139 - val_acc: 0.8539\n",
      "Epoch 162/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1374 - acc: 0.8192 - val_loss: 0.1206 - val_acc: 0.8427\n",
      "Epoch 163/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1348 - acc: 0.8092 - val_loss: 0.1218 - val_acc: 0.8427\n",
      "Epoch 164/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1447 - acc: 0.7955 - val_loss: 0.1169 - val_acc: 0.8315\n",
      "Epoch 165/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1319 - acc: 0.8192 - val_loss: 0.1133 - val_acc: 0.8427\n",
      "Epoch 166/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1335 - acc: 0.8080 - val_loss: 0.1148 - val_acc: 0.8427\n",
      "Epoch 167/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1324 - acc: 0.8192 - val_loss: 0.1148 - val_acc: 0.8427\n",
      "Epoch 168/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1299 - acc: 0.8242 - val_loss: 0.1162 - val_acc: 0.8539\n",
      "Epoch 169/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1292 - acc: 0.8229 - val_loss: 0.1166 - val_acc: 0.8427\n",
      "Epoch 170/250\n",
      "802/802 [==============================] - 0s 47us/step - loss: 0.1292 - acc: 0.8155 - val_loss: 0.1141 - val_acc: 0.8427\n",
      "Epoch 171/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1382 - acc: 0.7955 - val_loss: 0.1137 - val_acc: 0.8427\n",
      "Epoch 172/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1306 - acc: 0.8204 - val_loss: 0.1197 - val_acc: 0.8539\n",
      "Epoch 173/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1351 - acc: 0.8180 - val_loss: 0.1166 - val_acc: 0.8315\n",
      "Epoch 174/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1344 - acc: 0.8105 - val_loss: 0.1187 - val_acc: 0.8427\n",
      "Epoch 175/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1383 - acc: 0.8142 - val_loss: 0.1139 - val_acc: 0.8427\n",
      "Epoch 176/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1301 - acc: 0.8254 - val_loss: 0.1187 - val_acc: 0.8315\n",
      "Epoch 177/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1360 - acc: 0.8130 - val_loss: 0.1205 - val_acc: 0.8315\n",
      "Epoch 178/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1300 - acc: 0.8204 - val_loss: 0.1159 - val_acc: 0.8315\n",
      "Epoch 179/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1301 - acc: 0.8254 - val_loss: 0.1159 - val_acc: 0.8427\n",
      "Epoch 180/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1297 - acc: 0.8317 - val_loss: 0.1161 - val_acc: 0.8315\n",
      "Epoch 181/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "802/802 [==============================] - 0s 43us/step - loss: 0.1301 - acc: 0.8267 - val_loss: 0.1215 - val_acc: 0.8652\n",
      "Epoch 182/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1393 - acc: 0.8105 - val_loss: 0.1216 - val_acc: 0.8315\n",
      "Epoch 183/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1568 - acc: 0.7968 - val_loss: 0.1216 - val_acc: 0.8427\n",
      "Epoch 184/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1338 - acc: 0.8167 - val_loss: 0.1312 - val_acc: 0.8202\n",
      "Epoch 185/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1568 - acc: 0.7855 - val_loss: 0.1195 - val_acc: 0.8315\n",
      "Epoch 186/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1392 - acc: 0.8180 - val_loss: 0.1193 - val_acc: 0.8539\n",
      "Epoch 187/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1424 - acc: 0.8055 - val_loss: 0.1243 - val_acc: 0.7978\n",
      "Epoch 188/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1355 - acc: 0.8105 - val_loss: 0.1243 - val_acc: 0.8202\n",
      "Epoch 189/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1376 - acc: 0.8167 - val_loss: 0.1175 - val_acc: 0.8315\n",
      "Epoch 190/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1343 - acc: 0.8229 - val_loss: 0.1197 - val_acc: 0.8315\n",
      "Epoch 191/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1334 - acc: 0.8180 - val_loss: 0.1160 - val_acc: 0.8539\n",
      "Epoch 192/250\n",
      "802/802 [==============================] - 0s 43us/step - loss: 0.1321 - acc: 0.8142 - val_loss: 0.1178 - val_acc: 0.8427\n",
      "Epoch 193/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1324 - acc: 0.8229 - val_loss: 0.1204 - val_acc: 0.8539\n",
      "Epoch 194/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1355 - acc: 0.8180 - val_loss: 0.1165 - val_acc: 0.8427\n",
      "Epoch 195/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1301 - acc: 0.8242 - val_loss: 0.1200 - val_acc: 0.8202\n",
      "Epoch 196/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1266 - acc: 0.8279 - val_loss: 0.1169 - val_acc: 0.8315\n",
      "Epoch 197/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1294 - acc: 0.8192 - val_loss: 0.1181 - val_acc: 0.8315\n",
      "Epoch 198/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1304 - acc: 0.8204 - val_loss: 0.1194 - val_acc: 0.8427\n",
      "Epoch 199/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1354 - acc: 0.8155 - val_loss: 0.1242 - val_acc: 0.8090\n",
      "Epoch 200/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1353 - acc: 0.8229 - val_loss: 0.1204 - val_acc: 0.8315\n",
      "Epoch 201/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1328 - acc: 0.8180 - val_loss: 0.1170 - val_acc: 0.8427\n",
      "Epoch 202/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1339 - acc: 0.8204 - val_loss: 0.1146 - val_acc: 0.8315\n",
      "Epoch 203/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1272 - acc: 0.8267 - val_loss: 0.1157 - val_acc: 0.8427\n",
      "Epoch 204/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1370 - acc: 0.8130 - val_loss: 0.1202 - val_acc: 0.8427\n",
      "Epoch 205/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1291 - acc: 0.8267 - val_loss: 0.1213 - val_acc: 0.8315\n",
      "Epoch 206/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1298 - acc: 0.8167 - val_loss: 0.1162 - val_acc: 0.8427\n",
      "Epoch 207/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1368 - acc: 0.8192 - val_loss: 0.1169 - val_acc: 0.8427\n",
      "Epoch 208/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1266 - acc: 0.8267 - val_loss: 0.1203 - val_acc: 0.8539\n",
      "Epoch 209/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1279 - acc: 0.8155 - val_loss: 0.1172 - val_acc: 0.8315\n",
      "Epoch 210/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1442 - acc: 0.7968 - val_loss: 0.1273 - val_acc: 0.8427\n",
      "Epoch 211/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1361 - acc: 0.8180 - val_loss: 0.1184 - val_acc: 0.8315\n",
      "Epoch 212/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1325 - acc: 0.8229 - val_loss: 0.1174 - val_acc: 0.8539\n",
      "Epoch 213/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1468 - acc: 0.8080 - val_loss: 0.1184 - val_acc: 0.8202\n",
      "Epoch 214/250\n",
      "802/802 [==============================] - 0s 40us/step - loss: 0.1305 - acc: 0.8242 - val_loss: 0.1198 - val_acc: 0.8202\n",
      "Epoch 215/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1358 - acc: 0.8117 - val_loss: 0.1153 - val_acc: 0.8427\n",
      "Epoch 216/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1325 - acc: 0.8204 - val_loss: 0.1176 - val_acc: 0.8427\n",
      "Epoch 217/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1269 - acc: 0.8254 - val_loss: 0.1191 - val_acc: 0.8539\n",
      "Epoch 218/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1358 - acc: 0.8117 - val_loss: 0.1202 - val_acc: 0.8090\n",
      "Epoch 219/250\n",
      "802/802 [==============================] - 0s 57us/step - loss: 0.1305 - acc: 0.8192 - val_loss: 0.1182 - val_acc: 0.8315\n",
      "Epoch 220/250\n",
      "802/802 [==============================] - 0s 55us/step - loss: 0.1328 - acc: 0.8055 - val_loss: 0.1159 - val_acc: 0.8427\n",
      "Epoch 221/250\n",
      "802/802 [==============================] - 0s 47us/step - loss: 0.1309 - acc: 0.8117 - val_loss: 0.1153 - val_acc: 0.8202\n",
      "Epoch 222/250\n",
      "802/802 [==============================] - 0s 51us/step - loss: 0.1321 - acc: 0.8279 - val_loss: 0.1188 - val_acc: 0.8202\n",
      "Epoch 223/250\n",
      "802/802 [==============================] - 0s 56us/step - loss: 0.1287 - acc: 0.8217 - val_loss: 0.1190 - val_acc: 0.8652\n",
      "Epoch 224/250\n",
      "802/802 [==============================] - 0s 57us/step - loss: 0.1269 - acc: 0.8279 - val_loss: 0.1161 - val_acc: 0.8539\n",
      "Epoch 225/250\n",
      "802/802 [==============================] - 0s 57us/step - loss: 0.1359 - acc: 0.8055 - val_loss: 0.1162 - val_acc: 0.8202\n",
      "Epoch 226/250\n",
      "802/802 [==============================] - 0s 51us/step - loss: 0.1264 - acc: 0.8354 - val_loss: 0.1239 - val_acc: 0.8202\n",
      "Epoch 227/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1418 - acc: 0.8042 - val_loss: 0.1438 - val_acc: 0.8202\n",
      "Epoch 228/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1430 - acc: 0.8055 - val_loss: 0.1211 - val_acc: 0.8202\n",
      "Epoch 229/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1326 - acc: 0.8267 - val_loss: 0.1148 - val_acc: 0.8090\n",
      "Epoch 230/250\n",
      "802/802 [==============================] - 0s 42us/step - loss: 0.1287 - acc: 0.8292 - val_loss: 0.1171 - val_acc: 0.8202\n",
      "Epoch 231/250\n",
      "802/802 [==============================] - 0s 41us/step - loss: 0.1283 - acc: 0.8279 - val_loss: 0.1193 - val_acc: 0.8315\n",
      "Epoch 232/250\n",
      "802/802 [==============================] - 0s 47us/step - loss: 0.1340 - acc: 0.8180 - val_loss: 0.1173 - val_acc: 0.8202\n",
      "Epoch 233/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1276 - acc: 0.8229 - val_loss: 0.1175 - val_acc: 0.8202\n",
      "Epoch 234/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1351 - acc: 0.8155 - val_loss: 0.1166 - val_acc: 0.8315\n",
      "Epoch 235/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1258 - acc: 0.8304 - val_loss: 0.1186 - val_acc: 0.8315\n",
      "Epoch 236/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1366 - acc: 0.8167 - val_loss: 0.1193 - val_acc: 0.8315\n",
      "Epoch 237/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1277 - acc: 0.8192 - val_loss: 0.1201 - val_acc: 0.8202\n",
      "Epoch 238/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1242 - acc: 0.8317 - val_loss: 0.1205 - val_acc: 0.8202\n",
      "Epoch 239/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1326 - acc: 0.8130 - val_loss: 0.1194 - val_acc: 0.8315\n",
      "Epoch 240/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1250 - acc: 0.8367 - val_loss: 0.1257 - val_acc: 0.8652\n",
      "Epoch 241/250\n",
      "802/802 [==============================] - 0s 44us/step - loss: 0.1374 - acc: 0.8005 - val_loss: 0.1194 - val_acc: 0.8539\n",
      "Epoch 242/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1283 - acc: 0.8342 - val_loss: 0.1208 - val_acc: 0.8315\n",
      "Epoch 243/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1291 - acc: 0.8242 - val_loss: 0.1184 - val_acc: 0.8202\n",
      "Epoch 244/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1256 - acc: 0.8304 - val_loss: 0.1183 - val_acc: 0.8315\n",
      "Epoch 245/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1324 - acc: 0.8192 - val_loss: 0.1168 - val_acc: 0.8315\n",
      "Epoch 246/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1257 - acc: 0.8267 - val_loss: 0.1157 - val_acc: 0.8202\n",
      "Epoch 247/250\n",
      "802/802 [==============================] - 0s 46us/step - loss: 0.1281 - acc: 0.8279 - val_loss: 0.1167 - val_acc: 0.8539\n",
      "Epoch 248/250\n",
      "802/802 [==============================] - 0s 45us/step - loss: 0.1246 - acc: 0.8304 - val_loss: 0.1221 - val_acc: 0.8427\n",
      "Epoch 249/250\n",
      "802/802 [==============================] - 0s 52us/step - loss: 0.1271 - acc: 0.8192 - val_loss: 0.1203 - val_acc: 0.8090\n",
      "Epoch 250/250\n",
      "802/802 [==============================] - 0s 51us/step - loss: 0.1371 - acc: 0.8080 - val_loss: 0.1267 - val_acc: 0.8427\n",
      "\n",
      "> Entrenamiento realizado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Configuración de la red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=260, activation='relu', input_dim=5))   \n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(lr=0.001), metrics = ['accuracy'])\n",
    "\n",
    "datos_entrenamiento = model.fit(entrada_entrenamiento, salida_entrenamiento,  epochs=250, verbose=1, validation_data = (entrada_validacion, salida_validacion))\n",
    "\n",
    "print(\"\\n> Entrenamiento realizado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El score del conjunto de validación es de: 0.1267.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9255a6d0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(datos_entrenamiento.history['loss'], label='loss')\n",
    "plt.plot(datos_entrenamiento.history['val_loss'], label='val_loss')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "score = model.evaluate(entrada_validacion, salida_validacion, verbose = 0)\n",
    "\n",
    "print(\"El score del conjunto de validación es de: %.4f.\" % (score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación 2. Predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilidad de supervivencia: \n",
      "\n",
      "- Kelly, Mr. James: 3.58%\n",
      "- Wilkes, Mrs. James (Ellen Needs): 22.94%\n",
      "- Myles, Mr. Thomas Francis: 2.71%\n",
      "- Wirz, Mr. Albert: 4.02%\n",
      "- Hirvonen, Mrs. Alexander (Helga E Lindqvist): 42.21%\n",
      "- Svensson, Mr. Johan Cervin: 17.63%\n",
      "- Connolly, Miss. Kate: 58.53%\n",
      "- Caldwell, Mr. Albert Francis: 4.4%\n",
      "- Abrahim, Mrs. Joseph (Sophie Halaut Easu): 47.04%\n",
      "- Davies, Mr. John Samuel: 1.97%\n",
      "- Ilieff, Mr. Ylio: 3.57%\n",
      "- Jones, Mr. Charles Cresson: 13.2%\n",
      "- Snyder, Mrs. John Pillsbury (Nelle Stevenson): 95.26%\n",
      "- Howard, Mr. Benjamin: 1.39%\n",
      "- Chaffee, Mrs. Herbert Fuller (Carrie Constance Toogood): 90.83%\n",
      "- del Carlo, Mrs. Sebastiano (Argenia Genovesi): 60.72%\n",
      "- Keane, Mr. Daniel: 8.07%\n",
      "- Assaf, Mr. Gerios: 16.81%\n",
      "- Ilmakangas, Miss. Ida Livija: 41.94%\n",
      "- Assaf Khalil, Mrs. Mariana (Miriam\")\": 19.18%\n",
      "- Rothschild, Mr. Martin: 4.84%\n",
      "- Olsen, Master. Artur Karl: 62.67%\n",
      "- Flegenheim, Mrs. Alfred (Antoinette): 94.94%\n",
      "- Williams, Mr. Richard Norris II: 26.38%\n",
      "- Ryerson, Mrs. Arthur Larned (Emily Maria Borie): 26.04%\n",
      "- Robins, Mr. Alexander A: 1.01%\n",
      "- Ostby, Miss. Helene Ragnhild: 96.89%\n",
      "- Daher, Mr. Shedid: 15.04%\n",
      "- Brady, Mr. John Bertram: 16.4%\n",
      "- Samaan, Mr. Elias: 2.85%\n",
      "- Louch, Mr. Charles Alexander: 2.73%\n",
      "- Jefferys, Mr. Clifford Thomas: 4.37%\n",
      "- Dean, Mrs. Bertram (Eva Georgetta Light): 28.5%\n",
      "- Johnston, Mrs. Andrew G (Elizabeth Lily\" Watson)\": 20.53%\n",
      "- Mock, Mr. Philipp Edmund: 17.18%\n",
      "- Katavelas, Mr. Vassilios (Catavelas Vassilios\")\": 23.13%\n",
      "- Roth, Miss. Sarah A: 37.77%\n",
      "- Cacic, Miss. Manda: 47.3%\n",
      "- Sap, Mr. Julius: 4.75%\n",
      "- Hee, Mr. Ling: 0.17%\n",
      "- Karun, Mr. Franz: 1.6%\n",
      "- Franklin, Mr. Thomas Parham: 18.26%\n",
      "- Goldsmith, Mr. Nathan: 2.05%\n",
      "- Corbett, Mrs. Walter H (Irene Colvin): 80.71%\n",
      "- Kimball, Mrs. Edwin Nelson Jr (Gertrude Parsons): 91.06%\n",
      "- Peltomaki, Mr. Nikolai Johannes: 5.24%\n",
      "- Chevre, Mr. Paul Romaine: 13.94%\n",
      "- Shaughnessy, Mr. Patrick: 4.38%\n",
      "- Bucknell, Mrs. William Robert (Emma Eliza Ward): 79.57%\n",
      "- Coutts, Mrs. William (Winnie Minnie\" Treanor)\": 30.35%\n",
      "- Smith, Mr. Lucien Philip: 12.33%\n",
      "- Pulbaum, Mr. Franz: 11.66%\n",
      "- Hocking, Miss. Ellen Nellie\"\": 69.66%\n",
      "- Fortune, Miss. Ethel Flora: 77.41%\n",
      "- Mangiavacchi, Mr. Serafino Emilio: 7.98%\n",
      "- Rice, Master. Albert: 0.91%\n",
      "- Cor, Mr. Bartol: 2.76%\n",
      "- Abelseth, Mr. Olaus Jorgensen: 5.33%\n",
      "- Davison, Mr. Thomas Henry: 2.62%\n",
      "- Chaudanson, Miss. Victorine: 30.35%\n",
      "- Dika, Mr. Mirko: 12.9%\n",
      "- McCrae, Mr. Arthur Gordon: 7.1%\n",
      "- Bjorklund, Mr. Ernst Herbert: 11.64%\n",
      "- Bradley, Miss. Bridget Delia: 65.78%\n",
      "- Ryerson, Master. John Borie: 93.8%\n",
      "- Corey, Mrs. Percy C (Mary Phyllis Elizabeth Miller): 76.05%\n",
      "- Burns, Miss. Mary Delia: 66.31%\n",
      "- Moore, Mr. Clarence Bloomfield: 7.67%\n",
      "- Tucker, Mr. Gilbert Milligan Jr: 17.95%\n",
      "- Fortune, Mrs. Mark (Mary McDougald): 22.57%\n",
      "- Mulvihill, Miss. Bertha E: 64.58%\n",
      "- Minkoff, Mr. Lazar: 8.28%\n",
      "- Nieminen, Miss. Manta Josefina: 39.19%\n",
      "- Ovies y Rodriguez, Mr. Servando: 20.75%\n",
      "- Geiger, Miss. Amalie: 53.6%\n",
      "- Keeping, Mr. Edwin: 0.48%\n",
      "- Miles, Mr. Frank: 3.55%\n",
      "- Cornell, Mrs. Robert Clifford (Malvina Helen Lamson): 93.67%\n",
      "- Aldworth, Mr. Charles Augustus: 7.85%\n",
      "- Doyle, Miss. Elizabeth: 64.58%\n",
      "- Boulos, Master. Akar: 91.5%\n",
      "- Straus, Mr. Isidor: 0.32%\n",
      "- Case, Mr. Howard Brown: 11.12%\n",
      "- Demetri, Mr. Marinko: 3.57%\n",
      "- Lamb, Mr. John Joseph: 10.28%\n",
      "- Khalil, Mr. Betros: 4.04%\n",
      "- Barry, Miss. Julia: 61.65%\n",
      "- Badman, Miss. Emily Louisa: 49.52%\n",
      "- O'Donoghue, Ms. Bridget: 58.55%\n",
      "- Wells, Master. Ralph Lester: 78.5%\n",
      "- Dyker, Mrs. Adolf Fredrik (Anna Elisabeth Judith Andersson): 38.44%\n",
      "- Pedersen, Mr. Olaf: 3.58%\n",
      "- Davidson, Mrs. Thornton (Orian Hays): 95.49%\n",
      "- Guest, Mr. Robert: 3.55%\n",
      "- Birnbaum, Mr. Jakob: 24.54%\n",
      "- Tenglin, Mr. Gunnar Isidor: 5.28%\n",
      "- Cavendish, Mrs. Tyrell William (Julia Florence Siegel): 67.09%\n",
      "- Makinen, Mr. Kalle Edvard: 3.75%\n",
      "- Braf, Miss. Elin Ester Maria: 48.85%\n",
      "- Nancarrow, Mr. William Henry: 3.04%\n",
      "- Stengel, Mrs. Charles Emil Henry (Annie May Morris): 89.57%\n",
      "- Weisz, Mr. Leopold: 4.93%\n",
      "- Foley, Mr. William: 4.38%\n",
      "- Johansson Palmquist, Mr. Oskar Leander: 4.69%\n",
      "- Thomas, Mrs. Alexander (Thamine Thelma\")\": 50.73%\n",
      "- Holthen, Mr. Johan Martin: 1.98%\n",
      "- Buckley, Mr. Daniel: 6.49%\n",
      "- Ryan, Mr. Edward: 4.38%\n",
      "- Willer, Mr. Aaron (Abi Weller\")\": 3.46%\n",
      "- Swane, Mr. George: 17.03%\n",
      "- Stanton, Mr. Samuel Ward: 3.75%\n",
      "- Shine, Miss. Ellen Natalia: 58.55%\n",
      "- Evans, Miss. Edith Corse: 90.97%\n",
      "- Buckley, Miss. Katherine: 66.98%\n",
      "- Straus, Mrs. Isidor (Rosalie Ida Blun): 46.21%\n",
      "- Chronopoulos, Mr. Demetrios: 18.08%\n",
      "- Thomas, Mr. John: 5.74%\n",
      "- Sandstrom, Miss. Beatrice Irene: 61.78%\n",
      "- Beattie, Mr. Thomson: 10.42%\n",
      "- Chapman, Mrs. John Henry (Sara Elizabeth Lawry): 66.17%\n",
      "- Watt, Miss. Bertha J: 75.35%\n",
      "- Kiernan, Mr. John: 4.38%\n",
      "- Schabert, Mrs. Paul (Emma Mock): 93.64%\n",
      "- Carver, Mr. Alfred John: 4.03%\n",
      "- Kennedy, Mr. John: 4.38%\n",
      "- Cribb, Miss. Laura Alice: 25.53%\n",
      "- Brobeck, Mr. Karl Rudolf: 7.46%\n",
      "- McCoy, Miss. Alicia: 37.85%\n",
      "- Bowenur, Mr. Solomon: 4.48%\n",
      "- Petersen, Mr. Marius: 5.87%\n",
      "- Spinner, Mr. Henry John: 3.2%\n",
      "- Gracie, Col. Archibald IV: 9.32%\n",
      "- Lefebre, Mrs. Frank (Frances): 17.21%\n",
      "- Thomas, Mr. Charles P: 5.74%\n",
      "- Dintcheff, Mr. Valtcho: 1.86%\n",
      "- Carlsson, Mr. Carl Robert: 5.93%\n",
      "- Zakarian, Mr. Mapriededer: 9.37%\n",
      "- Schmidt, Mr. August: 9.64%\n",
      "- Drapkin, Miss. Jennie: 46.82%\n",
      "- Goodwin, Mr. Charles Frederick: 0.31%\n",
      "- Goodwin, Miss. Jessie Allis: 10.94%\n",
      "- Daniels, Miss. Sarah: 80.02%\n",
      "- Ryerson, Mr. Arthur Larned: 0.14%\n",
      "- Beauchamp, Mr. Henry James: 4.83%\n",
      "- Lindeberg-Lind, Mr. Erik Gustaf (Mr Edward Lingrey\")\": 15.94%\n",
      "- Vander Planke, Mr. Julius: 2.37%\n",
      "- Hilliard, Mr. Herbert Henry: 9.96%\n",
      "- Davies, Mr. Evan: 7.33%\n",
      "- Crafton, Mr. John Bertram: 18.26%\n",
      "- Lahtinen, Rev. William: 4.62%\n",
      "- Earnshaw, Mrs. Boulton (Olive Potter): 96.01%\n",
      "- Matinoff, Mr. Nicola: 5.41%\n",
      "- Storey, Mr. Thomas: 0.44%\n",
      "- Klasen, Mrs. (Hulda Kristina Eugenia Lofqvist): 30.5%\n",
      "- Asplund, Master. Filip Oscar: 2.63%\n",
      "- Duquemin, Mr. Joseph: 6.04%\n",
      "- Bird, Miss. Ellen: 58.1%\n",
      "- Lundin, Miss. Olga Elida: 47.03%\n",
      "- Borebank, Mr. John James: 15.94%\n",
      "- Peacock, Mrs. Benjamin (Edith Nile): 38.02%\n",
      "- Smyth, Miss. Julia: 58.55%\n",
      "- Touma, Master. Georges Youssef: 88.5%\n",
      "- Wright, Miss. Marion: 82.47%\n",
      "- Pearce, Mr. Ernest: 3.68%\n",
      "- Peruschitz, Rev. Joseph Maria: 4.69%\n",
      "- Kink-Heilmann, Mrs. Anton (Luise Heilmann): 20.11%\n",
      "- Brandeis, Mr. Emil: 7.12%\n",
      "- Ford, Mr. Edward Watson: 0.82%\n",
      "- Cassebeer, Mrs. Henry Arthur Jr (Eleanor Genevieve Fosdick): 92.68%\n",
      "- Hellstrom, Miss. Hilda Maria: 46.4%\n",
      "- Lithman, Mr. Simon: 3.61%\n",
      "- Zakarian, Mr. Ortin: 8.22%\n",
      "- Dyker, Mr. Adolf Fredrik: 4.55%\n",
      "- Torfa, Mr. Assad: 5.56%\n",
      "- Asplund, Mr. Carl Oscar Vilhelm Gustafsson: 0.93%\n",
      "- Brown, Miss. Edith Eileen: 65.55%\n",
      "- Sincock, Miss. Maude: 61.99%\n",
      "- Stengel, Mr. Charles Emil Henry: 4.93%\n",
      "- Becker, Mrs. Allen Oliver (Nellie E Baumgardner): 52.73%\n",
      "- Compton, Mrs. Alexander Taylor (Mary Eliza Ingersoll): 77.49%\n",
      "- McCrie, Mr. James Matthew: 7.85%\n",
      "- Compton, Mr. Alexander Taylor Jr: 9.19%\n",
      "- Marvin, Mrs. Daniel Warner (Mary Graham Carmichael Farquarson): 97.18%\n",
      "- Lane, Mr. Patrick: 4.38%\n",
      "- Douglas, Mrs. Frederick Charles (Mary Helene Baxter): 82.27%\n",
      "- Maybery, Mr. Frank Hubert: 4.75%\n",
      "- Phillips, Miss. Alice Frances Louisa: 69.2%\n",
      "- Davies, Mr. Joseph: 12.79%\n",
      "- Sage, Miss. Ada: 3.52%\n",
      "- Veal, Mr. James: 4.92%\n",
      "- Angle, Mr. William A: 4.97%\n",
      "- Salomon, Mr. Abraham L: 18.92%\n",
      "- van Billiard, Master. Walter John: 18.43%\n",
      "- Lingane, Mr. John: 2.61%\n",
      "- Drew, Master. Marshall Brines: 22.96%\n",
      "- Karlsson, Mr. Julius Konrad Eugen: 3.06%\n",
      "- Spedden, Master. Robert Douglas: 94.31%\n",
      "- Nilsson, Miss. Berta Olivia: 49.89%\n",
      "- Baimbrigge, Mr. Charles Robert: 11.79%\n",
      "- Rasmussen, Mrs. (Lena Jacobsen Solvang): 37.75%\n",
      "- Murphy, Miss. Nora: 56.61%\n",
      "- Danbom, Master. Gilbert Sigvard Emanuel: 82.1%\n",
      "- Astor, Col. John Jacob: 0.31%\n",
      "- Quick, Miss. Winifred Vera: 73.87%\n",
      "- Andrew, Mr. Frank Thomas: 10.75%\n",
      "- Omont, Mr. Alfred Fernand: 20.05%\n",
      "- McGowan, Miss. Katherine: 53.27%\n",
      "- Collett, Mr. Sidney C Stuart: 11.26%\n",
      "- Rosenbaum, Miss. Edith Louise: 92.67%\n",
      "- Delalic, Mr. Redjo: 5.25%\n",
      "- Andersen, Mr. Albert Karvin: 2.01%\n",
      "- Finoli, Mr. Luigi: 3.67%\n",
      "- Deacon, Mr. Percy William: 1.14%\n",
      "- Howard, Mrs. Benjamin (Ellen Truelove Arman): 56.17%\n",
      "- Andersson, Miss. Ida Augusta Margareta: 29.66%\n",
      "- Head, Mr. Christopher: 8.55%\n",
      "- Mahon, Miss. Bridget Delia: 58.57%\n",
      "- Wick, Mr. George Dennick: 1.01%\n",
      "- Widener, Mrs. George Dunton (Eleanor Elkins): 56.78%\n",
      "- Thomson, Mr. Alexander Morrison: 3.55%\n",
      "- Duran y More, Miss. Florentina: 68.8%\n",
      "- Reynolds, Mr. Harold J: 8.19%\n",
      "- Cook, Mrs. (Selena Rogers): 84.17%\n",
      "- Karlsson, Mr. Einar Gervasius: 8.34%\n",
      "- Candee, Mrs. Edward (Helen Churchill Hungerford): 88.39%\n",
      "- Moubarek, Mrs. George (Omine Amenia\" Alexander)\": 28.81%\n",
      "- Asplund, Mr. Johan Charles: 6.68%\n",
      "- McNeill, Miss. Bridget: 58.55%\n",
      "- Everett, Mr. Thomas James: 1.7%\n",
      "- Hocking, Mr. Samuel James Metcalfe: 5.96%\n",
      "- Sweet, Mr. George Frederick: 1.97%\n",
      "- Willard, Miss. Constance: 96.85%\n",
      "- Wiklund, Mr. Karl Johan: 9.28%\n",
      "- Linehan, Mr. Michael: 4.35%\n",
      "- Cumings, Mr. John Bradley: 9.15%\n",
      "- Vendel, Mr. Olof Edvin: 9.27%\n",
      "- Warren, Mr. Frank Manley: 3.05%\n",
      "- Baccos, Mr. Raffull: 18.74%\n",
      "- Hiltunen, Miss. Marta: 80.88%\n",
      "- Douglas, Mrs. Walter Donald (Mahala Dutton): 86.02%\n",
      "- Lindstrom, Mrs. Carl Johan (Sigrid Posse): 87.09%\n",
      "- Christy, Mrs. (Alice Frances): 68.36%\n",
      "- Spedden, Mr. Frederic Oakley: 3.12%\n",
      "- Hyman, Mr. Abraham: 3.57%\n",
      "- Johnston, Master. William Arthur Willie\"\": 1.83%\n",
      "- Kenyon, Mr. Frederick R: 9.08%\n",
      "- Karnes, Mrs. J Frank (Claire Bennett): 69.91%\n",
      "- Drew, Mr. James Vivian: 3.11%\n",
      "- Hold, Mrs. Stephen (Annie Margaret Hill): 66.17%\n",
      "- Khalil, Mrs. Betros (Zahie Maria\" Elias)\": 29.12%\n",
      "- West, Miss. Barbara J: 91.99%\n",
      "- Abrahamsson, Mr. Abraham August Johannes: 9.23%\n",
      "- Clark, Mr. Walter Miller: 3.44%\n",
      "- Salander, Mr. Karl Johan: 5.36%\n",
      "- Wenzel, Mr. Linhart: 3.04%\n",
      "- MacKay, Mr. George William: 3.61%\n",
      "- Mahon, Mr. John: 4.38%\n",
      "- Niklasson, Mr. Samuel: 3.92%\n",
      "- Bentham, Miss. Lilian W: 81.76%\n",
      "- Midtsjo, Mr. Karl Albert: 8.35%\n",
      "- de Messemaeker, Mr. Guillaume Joseph: 1.89%\n",
      "- Nilsson, Mr. August Ferdinand: 8.3%\n",
      "- Wells, Mrs. Arthur Henry (Addie\" Dart Trevaskis)\": 71.35%\n",
      "- Klasen, Miss. Gertrud Emilia: 73.4%\n",
      "- Portaluppi, Mr. Emilio Ilario Giuseppe: 8.89%\n",
      "- Lyntakoff, Mr. Stanko: 3.57%\n",
      "- Chisholm, Mr. Roderick Robert Crispin: 6.39%\n",
      "- Warren, Mr. Charles William: 3.61%\n",
      "- Howard, Miss. May Elizabeth: 37.77%\n",
      "- Pokrnic, Mr. Mate: 12.35%\n",
      "- McCaffry, Mr. Thomas Francis: 6.13%\n",
      "- Fox, Mr. Patrick: 4.38%\n",
      "- Clark, Mrs. Walter Miller (Virginia McDowell): 87.79%\n",
      "- Lennon, Miss. Mary: 56.61%\n",
      "- Saade, Mr. Jean Nassr: 5.56%\n",
      "- Bryhl, Miss. Dagmar Jenny Ingeborg : 70.24%\n",
      "- Parker, Mr. Clifford Richard: 9.16%\n",
      "- Faunthorpe, Mr. Harry: 4.31%\n",
      "- Ware, Mr. John James: 7.14%\n",
      "- Oxenham, Mr. Percy Thomas: 12.38%\n",
      "- Oreskovic, Miss. Jelka: 46.15%\n",
      "- Peacock, Master. Alfred Edward: 81.93%\n",
      "- Fleming, Miss. Honora: 58.55%\n",
      "- Touma, Miss. Maria Youssef: 72.03%\n",
      "- Rosblom, Miss. Salli Helena: 51.19%\n",
      "- Dennis, Mr. William: 2.7%\n",
      "- Franklin, Mr. Charles (Charles Fardon): 3.64%\n",
      "- Snyder, Mr. John Pillsbury: 8.17%\n",
      "- Mardirosian, Mr. Sarkis: 5.56%\n",
      "- Ford, Mr. Arthur: 3.55%\n",
      "- Rheims, Mr. George Alexander Lucien: 16.01%\n",
      "- Daly, Miss. Margaret Marcella Maggie\"\": 58.46%\n",
      "- Nasr, Mr. Mustafa: 5.56%\n",
      "- Dodge, Dr. Washington: 2.07%\n",
      "- Wittevrongel, Mr. Camille: 2.49%\n",
      "- Angheloff, Mr. Minko: 4.65%\n",
      "- Laroche, Miss. Louise: 98.01%\n",
      "- Samaan, Mr. Hanna: 2.85%\n",
      "- Loring, Mr. Joseph Holland: 12.2%\n",
      "- Johansson, Mr. Nils: 3.75%\n",
      "- Olsson, Mr. Oscar Wilhelm: 3.23%\n",
      "- Malachard, Mr. Noel: 8.17%\n",
      "- Phillips, Mr. Escott Robert: 3.92%\n",
      "- Pokrnic, Mr. Tome: 5.62%\n",
      "- McCarthy, Miss. Catherine Katie\"\": 58.55%\n",
      "- Crosby, Mrs. Edward Gifford (Catherine Elizabeth Halstead): 90.89%\n",
      "- Allison, Mr. Hudson Joshua Creighton: 1.42%\n",
      "- Aks, Master. Philip Frank: 89.56%\n",
      "- Hays, Mr. Charles Melville: 1.59%\n",
      "- Hansen, Mrs. Claus Peter (Jennie L Howard): 22.02%\n",
      "- Cacic, Mr. Jego Grga: 11.04%\n",
      "- Vartanian, Mr. David: 15.04%\n",
      "- Sadowitz, Mr. Harry: 3.61%\n",
      "- Carr, Miss. Jeannie: 51.24%\n",
      "- White, Mrs. John Stuart (Ella Holmes): 79.44%\n",
      "- Hagardon, Miss. Kate: 66.61%\n",
      "- Spencer, Mr. William Augustus: 1.43%\n",
      "- Rogers, Mr. Reginald Harry: 17.22%\n",
      "- Jonsson, Mr. Nils Hilding: 4.15%\n",
      "- Jefferys, Mr. Ernest Wilfred: 4.43%\n",
      "- Andersson, Mr. Johan Samuel: 4.69%\n",
      "- Krekorian, Mr. Neshan: 10.66%\n",
      "- Nesson, Mr. Israel: 9.64%\n",
      "- Rowe, Mr. Alfred G: 19.04%\n",
      "- Kreuchen, Miss. Emilie: 47.79%\n",
      "- Assam, Mr. Ali: 7.04%\n",
      "- Becker, Miss. Ruth Elizabeth: 66.56%\n",
      "- Rosenshine, Mr. George (Mr George Thorne\")\": 5.76%\n",
      "- Clarke, Mr. Charles Valentine: 4.69%\n",
      "- Enander, Mr. Ingvar: 12.39%\n",
      "- Davies, Mrs. John Morgan (Elizabeth Agnes Mary White) : 58.97%\n",
      "- Dulles, Mr. William Crothers: 17.2%\n",
      "- Thomas, Mr. Tannous: 5.56%\n",
      "- Nakid, Mrs. Said (Waika Mary\" Mowad)\": 30.58%\n",
      "- Cor, Mr. Ivan: 4.14%\n",
      "- Maguire, Mr. John Edward: 18.92%\n",
      "- de Brito, Mr. Jose Joaquim: 7.15%\n",
      "- Elias, Mr. Joseph: 1.91%\n",
      "- Denbury, Mr. Herbert: 4.32%\n",
      "- Betros, Master. Seman: 5.56%\n",
      "- Fillbrook, Mr. Joseph Charles: 19.16%\n",
      "- Lundstrom, Mr. Thure Edvin: 3.25%\n",
      "- Sage, Mr. John George: 0.09%\n",
      "- Cardeza, Mrs. James Warburton Martinez (Charlotte Wardle Drake): 4.98%\n",
      "- van Billiard, Master. James William: 2.74%\n",
      "- Abelseth, Miss. Karen Marie: 50.84%\n",
      "- Botsford, Mr. William Hull: 9.64%\n",
      "- Whabee, Mrs. George Joseph (Shawneene Abi-Saab): 25.27%\n",
      "- Giles, Mr. Ralph: 10.48%\n",
      "- Walcroft, Miss. Nellie: 76.9%\n",
      "- Greenfield, Mrs. Leo David (Blanche Strouse): 89.76%\n",
      "- Stokes, Mr. Philip Joseph: 10.75%\n",
      "- Dibden, Mr. William: 1.11%\n",
      "- Herman, Mr. Samuel: 0.7%\n",
      "- Dean, Miss. Elizabeth Gladys Millvina\"\": 57.79%\n",
      "- Julian, Mr. Henry Forbes: 10.48%\n",
      "- Brown, Mrs. John Murray (Caroline Lane Lamson): 87.73%\n",
      "- Lockyer, Mr. Edward: 3.57%\n",
      "- O'Keefe, Mr. Patrick: 4.38%\n",
      "- Lindell, Mrs. Edvard Bengtsson (Elin Gerda Persson): 34.08%\n",
      "- Sage, Master. William Henry: 0.33%\n",
      "- Mallet, Mrs. Albert (Antoinette Magnin): 62.09%\n",
      "- Ware, Mrs. John James (Florence Louise Long): 76.9%\n",
      "- Strilic, Mr. Ivan: 4.02%\n",
      "- Harder, Mrs. George Achilles (Dorothy Annan): 96.31%\n",
      "- Sage, Mrs. John (Annie Bullen): 3.52%\n",
      "- Caram, Mr. Joseph: 4.04%\n",
      "- Riihivouri, Miss. Susanna Juhantytar Sanni\"\": 8.76%\n",
      "- Gibson, Mrs. Leonard (Pauline C Boeson): 89.16%\n",
      "- Pallas y Castello, Mr. Emilio: 9.71%\n",
      "- Giles, Mr. Edgar: 13.22%\n",
      "- Wilson, Miss. Helen Alice: 88.89%\n",
      "- Ismay, Mr. Joseph Bruce: 0.83%\n",
      "- Harbeck, Mr. William H: 4.1%\n",
      "- Dodge, Mrs. Washington (Ruth Vidaver): 82.48%\n",
      "- Bowen, Miss. Grace Scott: 25.56%\n",
      "- Kink, Miss. Maria: 46.8%\n",
      "- Cotterill, Mr. Henry Harry\"\": 13.22%\n",
      "- Hipkins, Mr. William Edward: 4.91%\n",
      "- Asplund, Master. Carl Edgar: 23.72%\n",
      "- O'Connor, Mr. Patrick: 4.38%\n",
      "- Foley, Mr. Joseph: 5.32%\n",
      "- Risien, Mrs. Samuel (Emma): 35.15%\n",
      "- McNamee, Mrs. Neal (Eileen O'Leary): 27.6%\n",
      "- Wheeler, Mr. Edwin Frederick\"\": 7.87%\n",
      "- Herman, Miss. Kate: 52.15%\n",
      "- Aronsson, Mr. Ernst Axel Algot: 5.96%\n",
      "- Ashby, Mr. John: 2.33%\n",
      "- Canavan, Mr. Patrick: 6.53%\n",
      "- Palsson, Master. Paul Folke: 38.46%\n",
      "- Payne, Mr. Vivian Ponsonby: 5.74%\n",
      "- Lines, Mrs. Ernest H (Elizabeth Lindsey James): 93.45%\n",
      "- Abbott, Master. Eugene Joseph: 8.1%\n",
      "- Gilbert, Mr. William: 3.83%\n",
      "- Kink-Heilmann, Mr. Anton: 2.13%\n",
      "- Smith, Mrs. Lucien Philip (Mary Eloise Hughes): 96.94%\n",
      "- Colbert, Mr. Patrick: 6.04%\n",
      "- Frolicher-Stehli, Mrs. Maxmillian (Margaretha Emerentia Stehli): 87.03%\n",
      "- Larsson-Rondberg, Mr. Edvard A: 7.47%\n",
      "- Conlon, Mr. Thomas Henry: 4.18%\n",
      "- Bonnell, Miss. Caroline: 73.51%\n",
      "- Gale, Mr. Harry: 5.02%\n",
      "- Gibson, Miss. Dorothy Winifred: 96.9%\n",
      "- Carrau, Mr. Jose Pedro: 20.38%\n",
      "- Frauenthal, Mr. Isaac Gerald: 14.85%\n",
      "- Nourney, Mr. Alfred (Baron von Drachstedt\")\": 25.97%\n",
      "- Ware, Mr. William Jeffery: 11.79%\n",
      "- Widener, Mr. George Dunton: 0.48%\n",
      "- Riordan, Miss. Johanna Hannah\"\": 58.54%\n",
      "- Peacock, Miss. Treasteall: 65.74%\n",
      "- Naughton, Miss. Hannah: 58.55%\n",
      "- Minahan, Mrs. William Edward (Lillian E Thorpe): 90.65%\n",
      "- Henriksson, Miss. Jenny Lovisa: 40.62%\n",
      "- Spector, Mr. Woolf: 3.55%\n",
      "- Oliva y Ocana, Dona. Fermina: 91.34%\n",
      "- Saether, Mr. Simon Sivertsen: 2.45%\n",
      "- Ware, Mr. Frederick: 3.55%\n",
      "- Peter, Master. Michael J: 2.72%\n",
      "\n",
      "Número total de datos:  418\n",
      "Número de mujeres:  152\n",
      "Número de hombres:  266\n",
      "Número de niños:  32\n",
      "Número pasajeros primera clase:  1\n",
      "\n",
      "La media de probabilidad del dataset divido es:  28.320501\n"
     ]
    }
   ],
   "source": [
    "print(\"Probabilidad de supervivencia: \\n\")\n",
    "\n",
    "probA =  model.predict(entrada_pre) * 100\n",
    "mediaA = probA.mean()\n",
    "\n",
    "Imprimir_Prediccion_Vida(datos_pre['Name'], datos_pre['Sex'], datos_pre['Age'], datos_pre['Fare'], probA)\n",
    "\n",
    "print(\"\\nLa media de probabilidad del dataset divido es: \", mediaA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorización de los pesos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos medios por entrada\n",
      "------------------------\n",
      " Característica    Peso \n",
      "------------------------\n",
      "- Pclass: [-0.04314933]\n",
      "- Sex: [0.01322198]\n",
      "- Age: [-0.02370228]\n",
      "- Fare: [-0.0159891]\n",
      "- Embarked: [-0.02291574]\n"
     ]
    }
   ],
   "source": [
    "pesos_entrada = model.get_weights()[0]\n",
    "\n",
    "nombres_entrada = np.array(['Pclass','Sex','Age','Fare',\n",
    "                            'Embarked',])\n",
    "\n",
    "medias = np.array([ [np.average(pesos_entrada[0])], [np.average(pesos_entrada[1])], [np.average(pesos_entrada[2])],\n",
    "                    [np.average(pesos_entrada[3])], [np.average(pesos_entrada[4])]])\n",
    "\n",
    "print(\"Pesos medios por entrada\")\n",
    "print(\"------------------------\")\n",
    "print(\" Característica    Peso \")\n",
    "print(\"------------------------\")\n",
    "\n",
    "Imprimir_Medias_Pesos(nombres_entrada, medias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo del error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 891 samples, validate on 0 samples\n",
      "Epoch 1/250\n",
      "891/891 [==============================] - 0s 416us/step - loss: 0.2361 - acc: 0.6700\n",
      "Epoch 2/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.2212 - acc: 0.6723\n",
      "Epoch 3/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1949 - acc: 0.7104\n",
      "Epoch 4/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1892 - acc: 0.7071\n",
      "Epoch 5/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1773 - acc: 0.7363\n",
      "Epoch 6/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1778 - acc: 0.7363\n",
      "Epoch 7/250\n",
      "891/891 [==============================] - 0s 41us/step - loss: 0.1730 - acc: 0.7441\n",
      "Epoch 8/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1597 - acc: 0.7767\n",
      "Epoch 9/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1587 - acc: 0.7733\n",
      "Epoch 10/250\n",
      "891/891 [==============================] - 0s 51us/step - loss: 0.1605 - acc: 0.7688\n",
      "Epoch 11/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1571 - acc: 0.7722\n",
      "Epoch 12/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1524 - acc: 0.7924\n",
      "Epoch 13/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1497 - acc: 0.7823\n",
      "Epoch 14/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1469 - acc: 0.7834\n",
      "Epoch 15/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1500 - acc: 0.7868\n",
      "Epoch 16/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1473 - acc: 0.7901\n",
      "Epoch 17/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1463 - acc: 0.7912\n",
      "Epoch 18/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1548 - acc: 0.7811\n",
      "Epoch 19/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1512 - acc: 0.7890\n",
      "Epoch 20/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1522 - acc: 0.7868\n",
      "Epoch 21/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1509 - acc: 0.7800\n",
      "Epoch 22/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1463 - acc: 0.7924\n",
      "Epoch 23/250\n",
      "891/891 [==============================] - 0s 47us/step - loss: 0.1454 - acc: 0.7901\n",
      "Epoch 24/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1431 - acc: 0.8002\n",
      "Epoch 25/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1587 - acc: 0.7767\n",
      "Epoch 26/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1555 - acc: 0.7789\n",
      "Epoch 27/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1473 - acc: 0.7879\n",
      "Epoch 28/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1447 - acc: 0.8002\n",
      "Epoch 29/250\n",
      "891/891 [==============================] - 0s 46us/step - loss: 0.1564 - acc: 0.7800\n",
      "Epoch 30/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1514 - acc: 0.7912\n",
      "Epoch 31/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1462 - acc: 0.7856\n",
      "Epoch 32/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1403 - acc: 0.7980\n",
      "Epoch 33/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1484 - acc: 0.8036\n",
      "Epoch 34/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1506 - acc: 0.7935\n",
      "Epoch 35/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1438 - acc: 0.8126\n",
      "Epoch 36/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1397 - acc: 0.8036\n",
      "Epoch 37/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1489 - acc: 0.7901\n",
      "Epoch 38/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1448 - acc: 0.7935\n",
      "Epoch 39/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1571 - acc: 0.7800\n",
      "Epoch 40/250\n",
      "891/891 [==============================] - 0s 47us/step - loss: 0.1547 - acc: 0.7856\n",
      "Epoch 41/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1467 - acc: 0.7991\n",
      "Epoch 42/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1470 - acc: 0.8025\n",
      "Epoch 43/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1411 - acc: 0.8114\n",
      "Epoch 44/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1574 - acc: 0.7733\n",
      "Epoch 45/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1612 - acc: 0.7666\n",
      "Epoch 46/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1399 - acc: 0.8148\n",
      "Epoch 47/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1382 - acc: 0.8070\n",
      "Epoch 48/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1447 - acc: 0.7924\n",
      "Epoch 49/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1422 - acc: 0.8047\n",
      "Epoch 50/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1439 - acc: 0.8036\n",
      "Epoch 51/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1465 - acc: 0.7901\n",
      "Epoch 52/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1395 - acc: 0.8070\n",
      "Epoch 53/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1396 - acc: 0.8159\n",
      "Epoch 54/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1356 - acc: 0.8148\n",
      "Epoch 55/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1424 - acc: 0.8036\n",
      "Epoch 56/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1380 - acc: 0.8148\n",
      "Epoch 57/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1403 - acc: 0.8036\n",
      "Epoch 58/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1410 - acc: 0.7991\n",
      "Epoch 59/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1442 - acc: 0.7957\n",
      "Epoch 60/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1490 - acc: 0.7991\n",
      "Epoch 61/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1371 - acc: 0.8159\n",
      "Epoch 62/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1367 - acc: 0.8171\n",
      "Epoch 63/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1380 - acc: 0.8047\n",
      "Epoch 64/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1398 - acc: 0.8103\n",
      "Epoch 65/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1341 - acc: 0.8204\n",
      "Epoch 66/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1372 - acc: 0.8114\n",
      "Epoch 67/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1374 - acc: 0.8126\n",
      "Epoch 68/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1340 - acc: 0.8148\n",
      "Epoch 69/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1422 - acc: 0.8047\n",
      "Epoch 70/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1334 - acc: 0.8159\n",
      "Epoch 71/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1382 - acc: 0.8126\n",
      "Epoch 72/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1353 - acc: 0.8159\n",
      "Epoch 73/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1344 - acc: 0.8126\n",
      "Epoch 74/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1338 - acc: 0.8227\n",
      "Epoch 75/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1415 - acc: 0.7980\n",
      "Epoch 76/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1363 - acc: 0.8103\n",
      "Epoch 77/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1410 - acc: 0.8081\n",
      "Epoch 78/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1352 - acc: 0.8238\n",
      "Epoch 79/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1353 - acc: 0.8137\n",
      "Epoch 80/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1395 - acc: 0.8070\n",
      "Epoch 81/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1362 - acc: 0.8148\n",
      "Epoch 82/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1484 - acc: 0.7901\n",
      "Epoch 83/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1429 - acc: 0.8036\n",
      "Epoch 84/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1311 - acc: 0.8204\n",
      "Epoch 85/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1321 - acc: 0.8204\n",
      "Epoch 86/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1395 - acc: 0.8058\n",
      "Epoch 87/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1332 - acc: 0.8148\n",
      "Epoch 88/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1315 - acc: 0.8215\n",
      "Epoch 89/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1318 - acc: 0.8227\n",
      "Epoch 90/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1316 - acc: 0.8215\n",
      "Epoch 91/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1355 - acc: 0.8137\n",
      "Epoch 92/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1312 - acc: 0.8249\n",
      "Epoch 93/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1293 - acc: 0.8249\n",
      "Epoch 94/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1315 - acc: 0.8305\n",
      "Epoch 95/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1384 - acc: 0.8159\n",
      "Epoch 96/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1401 - acc: 0.8002\n",
      "Epoch 97/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1307 - acc: 0.8272\n",
      "Epoch 98/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1342 - acc: 0.8215\n",
      "Epoch 99/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1372 - acc: 0.8126\n",
      "Epoch 100/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1342 - acc: 0.8193\n",
      "Epoch 101/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1315 - acc: 0.8227\n",
      "Epoch 102/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1398 - acc: 0.8137\n",
      "Epoch 103/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1296 - acc: 0.8238\n",
      "Epoch 104/250\n",
      "891/891 [==============================] - 0s 47us/step - loss: 0.1376 - acc: 0.8103\n",
      "Epoch 105/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1360 - acc: 0.8103\n",
      "Epoch 106/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1295 - acc: 0.8193\n",
      "Epoch 107/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1345 - acc: 0.8182\n",
      "Epoch 108/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1326 - acc: 0.8260\n",
      "Epoch 109/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1350 - acc: 0.8182\n",
      "Epoch 110/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1346 - acc: 0.8193\n",
      "Epoch 111/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1330 - acc: 0.8081\n",
      "Epoch 112/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1360 - acc: 0.8126\n",
      "Epoch 113/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1315 - acc: 0.8227\n",
      "Epoch 114/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1380 - acc: 0.8081\n",
      "Epoch 115/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1396 - acc: 0.8047\n",
      "Epoch 116/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1335 - acc: 0.8215\n",
      "Epoch 117/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1318 - acc: 0.8148\n",
      "Epoch 118/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1289 - acc: 0.8215\n",
      "Epoch 119/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1305 - acc: 0.8148\n",
      "Epoch 120/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1359 - acc: 0.8159\n",
      "Epoch 121/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1406 - acc: 0.8092\n",
      "Epoch 122/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1337 - acc: 0.8171\n",
      "Epoch 123/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1289 - acc: 0.8238\n",
      "Epoch 124/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1300 - acc: 0.8204\n",
      "Epoch 125/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1427 - acc: 0.7980\n",
      "Epoch 126/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1339 - acc: 0.8159\n",
      "Epoch 127/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1375 - acc: 0.8171\n",
      "Epoch 128/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1324 - acc: 0.8171\n",
      "Epoch 129/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1354 - acc: 0.8114\n",
      "Epoch 130/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1351 - acc: 0.8204\n",
      "Epoch 131/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1317 - acc: 0.8238\n",
      "Epoch 132/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1335 - acc: 0.8171\n",
      "Epoch 133/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1335 - acc: 0.8159\n",
      "Epoch 134/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1274 - acc: 0.8260\n",
      "Epoch 135/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1329 - acc: 0.8227\n",
      "Epoch 136/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1359 - acc: 0.8058\n",
      "Epoch 137/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1305 - acc: 0.8227\n",
      "Epoch 138/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1278 - acc: 0.8227\n",
      "Epoch 139/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1310 - acc: 0.8103\n",
      "Epoch 140/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1303 - acc: 0.8294\n",
      "Epoch 141/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1317 - acc: 0.8103\n",
      "Epoch 142/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1346 - acc: 0.8058\n",
      "Epoch 143/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1335 - acc: 0.8249\n",
      "Epoch 144/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1317 - acc: 0.8193\n",
      "Epoch 145/250\n",
      "891/891 [==============================] - 0s 48us/step - loss: 0.1373 - acc: 0.8081\n",
      "Epoch 146/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1363 - acc: 0.8126\n",
      "Epoch 147/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1356 - acc: 0.8171\n",
      "Epoch 148/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1268 - acc: 0.8249\n",
      "Epoch 149/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1366 - acc: 0.8126\n",
      "Epoch 150/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1310 - acc: 0.8159\n",
      "Epoch 151/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1322 - acc: 0.8171\n",
      "Epoch 152/250\n",
      "891/891 [==============================] - 0s 47us/step - loss: 0.1276 - acc: 0.8215\n",
      "Epoch 153/250\n",
      "891/891 [==============================] - 0s 54us/step - loss: 0.1332 - acc: 0.8126\n",
      "Epoch 154/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1317 - acc: 0.8204\n",
      "Epoch 155/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1350 - acc: 0.8238\n",
      "Epoch 156/250\n",
      "891/891 [==============================] - 0s 36us/step - loss: 0.1309 - acc: 0.8294\n",
      "Epoch 157/250\n",
      "891/891 [==============================] - 0s 35us/step - loss: 0.1304 - acc: 0.8294\n",
      "Epoch 158/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1294 - acc: 0.8283\n",
      "Epoch 159/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1346 - acc: 0.8103\n",
      "Epoch 160/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1335 - acc: 0.8171\n",
      "Epoch 161/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1276 - acc: 0.8361\n",
      "Epoch 162/250\n",
      "891/891 [==============================] - 0s 48us/step - loss: 0.1273 - acc: 0.8215\n",
      "Epoch 163/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1257 - acc: 0.8249\n",
      "Epoch 164/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1338 - acc: 0.8058\n",
      "Epoch 165/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1325 - acc: 0.8171\n",
      "Epoch 166/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1266 - acc: 0.8294\n",
      "Epoch 167/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891/891 [==============================] - 0s 40us/step - loss: 0.1279 - acc: 0.8215\n",
      "Epoch 168/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1248 - acc: 0.8260\n",
      "Epoch 169/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1403 - acc: 0.8058\n",
      "Epoch 170/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1361 - acc: 0.8126\n",
      "Epoch 171/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1244 - acc: 0.8283\n",
      "Epoch 172/250\n",
      "891/891 [==============================] - 0s 53us/step - loss: 0.1322 - acc: 0.8249\n",
      "Epoch 173/250\n",
      "891/891 [==============================] - 0s 47us/step - loss: 0.1289 - acc: 0.8316\n",
      "Epoch 174/250\n",
      "891/891 [==============================] - 0s 57us/step - loss: 0.1264 - acc: 0.8227\n",
      "Epoch 175/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1346 - acc: 0.8215\n",
      "Epoch 176/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1260 - acc: 0.8294\n",
      "Epoch 177/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1256 - acc: 0.8227\n",
      "Epoch 178/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1250 - acc: 0.8215\n",
      "Epoch 179/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1292 - acc: 0.8215\n",
      "Epoch 180/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1388 - acc: 0.8013\n",
      "Epoch 181/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1348 - acc: 0.8103\n",
      "Epoch 182/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1280 - acc: 0.8215\n",
      "Epoch 183/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1247 - acc: 0.8328\n",
      "Epoch 184/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1283 - acc: 0.8204\n",
      "Epoch 185/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1280 - acc: 0.8249\n",
      "Epoch 186/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1296 - acc: 0.8238\n",
      "Epoch 187/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1319 - acc: 0.8114\n",
      "Epoch 188/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1294 - acc: 0.8272\n",
      "Epoch 189/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1303 - acc: 0.8182\n",
      "Epoch 190/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1313 - acc: 0.8204\n",
      "Epoch 191/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1314 - acc: 0.8182\n",
      "Epoch 192/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1254 - acc: 0.8260\n",
      "Epoch 193/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1334 - acc: 0.8227\n",
      "Epoch 194/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1320 - acc: 0.8204\n",
      "Epoch 195/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1281 - acc: 0.8193\n",
      "Epoch 196/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1257 - acc: 0.8215\n",
      "Epoch 197/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1305 - acc: 0.8305\n",
      "Epoch 198/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1334 - acc: 0.8227\n",
      "Epoch 199/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1348 - acc: 0.8159\n",
      "Epoch 200/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1325 - acc: 0.8193\n",
      "Epoch 201/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1400 - acc: 0.8002\n",
      "Epoch 202/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1248 - acc: 0.8305\n",
      "Epoch 203/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1249 - acc: 0.8283\n",
      "Epoch 204/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1241 - acc: 0.8339\n",
      "Epoch 205/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1274 - acc: 0.8204\n",
      "Epoch 206/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1258 - acc: 0.8260\n",
      "Epoch 207/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1277 - acc: 0.8249\n",
      "Epoch 208/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1278 - acc: 0.8249\n",
      "Epoch 209/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1258 - acc: 0.8294\n",
      "Epoch 210/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1258 - acc: 0.8272\n",
      "Epoch 211/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1288 - acc: 0.8260\n",
      "Epoch 212/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1270 - acc: 0.8227\n",
      "Epoch 213/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1288 - acc: 0.8171\n",
      "Epoch 214/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1286 - acc: 0.8227\n",
      "Epoch 215/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1272 - acc: 0.8171\n",
      "Epoch 216/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1272 - acc: 0.8238\n",
      "Epoch 217/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1314 - acc: 0.8092\n",
      "Epoch 218/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1369 - acc: 0.8103\n",
      "Epoch 219/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1336 - acc: 0.8193\n",
      "Epoch 220/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1307 - acc: 0.8182\n",
      "Epoch 221/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1389 - acc: 0.8092\n",
      "Epoch 222/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1548 - acc: 0.7879\n",
      "Epoch 223/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1294 - acc: 0.8215\n",
      "Epoch 224/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1336 - acc: 0.8159\n",
      "Epoch 225/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1338 - acc: 0.8204\n",
      "Epoch 226/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1253 - acc: 0.8294\n",
      "Epoch 227/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1385 - acc: 0.8126\n",
      "Epoch 228/250\n",
      "891/891 [==============================] - 0s 45us/step - loss: 0.1310 - acc: 0.8294\n",
      "Epoch 229/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1273 - acc: 0.8305\n",
      "Epoch 230/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1250 - acc: 0.8294\n",
      "Epoch 231/250\n",
      "891/891 [==============================] - 0s 42us/step - loss: 0.1221 - acc: 0.8283\n",
      "Epoch 232/250\n",
      "891/891 [==============================] - 0s 43us/step - loss: 0.1275 - acc: 0.8148\n",
      "Epoch 233/250\n",
      "891/891 [==============================] - 0s 44us/step - loss: 0.1303 - acc: 0.8272\n",
      "Epoch 234/250\n",
      "891/891 [==============================] - 0s 40us/step - loss: 0.1266 - acc: 0.8361\n",
      "Epoch 235/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1302 - acc: 0.8171\n",
      "Epoch 236/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1294 - acc: 0.8305\n",
      "Epoch 237/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1332 - acc: 0.8148\n",
      "Epoch 238/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1242 - acc: 0.8328\n",
      "Epoch 239/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1242 - acc: 0.8283\n",
      "Epoch 240/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1270 - acc: 0.8227\n",
      "Epoch 241/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1286 - acc: 0.8137\n",
      "Epoch 242/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1296 - acc: 0.8204\n",
      "Epoch 243/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1258 - acc: 0.8283\n",
      "Epoch 244/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1328 - acc: 0.8159\n",
      "Epoch 245/250\n",
      "891/891 [==============================] - 0s 38us/step - loss: 0.1289 - acc: 0.8171\n",
      "Epoch 246/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1254 - acc: 0.8395\n",
      "Epoch 247/250\n",
      "891/891 [==============================] - 0s 36us/step - loss: 0.1284 - acc: 0.8249\n",
      "Epoch 248/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1238 - acc: 0.8316\n",
      "Epoch 249/250\n",
      "891/891 [==============================] - 0s 37us/step - loss: 0.1259 - acc: 0.8193\n",
      "Epoch 250/250\n",
      "891/891 [==============================] - 0s 39us/step - loss: 0.1258 - acc: 0.8272\n",
      "\n",
      "> Entrenamiento realizado con éxito.\n"
     ]
    }
   ],
   "source": [
    "# Obtención del fichero de datos de entrenamiento y validación\n",
    "datos_error = pd.read_csv('../Datos/titanic.csv')\n",
    "\n",
    "# Recolección de información de entrada para la red neuronal (especificada en el enunciado)\n",
    "clase_error = datos_error.loc[:,'Pclass']\n",
    "genero_error = datos_error.loc[:,'Sex']\n",
    "edad_error = datos_error.loc[:,'Age']\n",
    "tarifa_error = datos_error.loc[:,'Fare']\n",
    "puerto_error = datos_error.loc[:,'Embarked']\n",
    "\n",
    "# Eliminamos las casillas donde no existe el dato de la edad\n",
    "# Hacemos el .roud() debido a que la media de la edad sale con muchos decimales\n",
    "\n",
    "edad_error = datos_error['Age'].fillna(datos_error['Age'].mean()).round()\n",
    "\n",
    "# Convertimos \"male\" en 0 y \"female\" en 1 para poder hacer el entrenamiento\n",
    "genero_error = genero_error.replace('male', 0)\n",
    "genero_error = genero_error.replace('female', 1)\n",
    "\n",
    "# Convertiomos los datos de los puertos según estos datos:\n",
    "# Q ==> 0\n",
    "# S ==> 1\n",
    "# C ==> 2\n",
    "puerto_error = puerto_error.replace('Q', 0)\n",
    "puerto_error = puerto_error.replace('S', 1)\n",
    "puerto_error = puerto_error.replace('C', 2)\n",
    "\n",
    "# Unión de la información de entrada\n",
    "entrada_error = np.array([[clase_error],[genero_error],[edad_error],[tarifa_error],[puerto_error]])\n",
    "\n",
    "# Cambiamos el tamaño de la matriz\n",
    "entrada_error = entrada_error.transpose().reshape(891, 5)\n",
    "\n",
    "# Recolección de la salida esperada de la red neuronal\n",
    "salida_esperada_error = np.array(datos_error.loc[:,'Survived'])\n",
    "\n",
    "# Conjunto entrenamiento\n",
    "entrada_entrenamiento_error = entrada_error[:891,:]\n",
    "salida_entrenamiento_error = salida_esperada_error[:891]\n",
    "\n",
    "# Conjunto validación\n",
    "entrada_validacion_error = entrada_error[891:,:]\n",
    "salida_validacion_error = salida_esperada_error[891:]\n",
    "\n",
    "\n",
    "# Configuración de la red neuronal\n",
    "model = Sequential()\n",
    "model.add(Dense(units=260, activation='relu', input_dim=5))   \n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(lr=0.001), metrics = ['accuracy'])\n",
    "\n",
    "datos_entrenamiento_error = model.fit(entrada_entrenamiento_error, salida_entrenamiento_error,  epochs=250, verbose=1, validation_data = (entrada_validacion_error, salida_validacion_error))\n",
    "\n",
    "print(\"\\n> Entrenamiento realizado con éxito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "La media de probabilidad del dataset completo es de:  37.080807\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probB = model.predict(entrada_error) * 100\n",
    "mediaB = probB.mean()\n",
    "\n",
    "print(\"\\nLa media de probabilidad del dataset completo es de: \", mediaB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El error en la predicción es del 8.760305%\n"
     ]
    }
   ],
   "source": [
    "error = abs(mediaA-mediaB)\n",
    "print(\"El error en la predicción es del \"+ str(error) + \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
